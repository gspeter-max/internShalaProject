{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['GROQ_API_KEY'] = '.........'"
      ],
      "metadata": {
        "id": "0CH6bXovNnxs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wvG2J-Gn1bu",
        "outputId": "8abd99d5-2d72-4688-f37f-f6fda8e1f7b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'internShalaProject'...\n",
            "remote: Enumerating objects: 177, done.\u001b[K\n",
            "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
            "remote: Compressing objects: 100% (156/156), done.\u001b[K\n",
            "remote: Total 177 (delta 43), reused 31 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (177/177), 2.63 MiB | 15.64 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/gspeter-max/internShalaProject.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAZAUn7Nn9tF",
        "outputId": "3fcb1fda-35e3-47e7-cdad-03e98d17e088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/internShalaProject/Yardstick_Assignment\n"
          ]
        }
      ],
      "source": [
        "%cd /content/internShalaProject/Yardstick_Assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-ALGKTNoGC-",
        "outputId": "0f049c8d-b055-441f-98f4-61ba4f6f281b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/internShalaProject/Yardstick_Assignment\n"
          ]
        }
      ],
      "source": [
        "! pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLsDgO6KoZrV",
        "outputId": "61450527-0dc9-438f-e347-65df9bfb8f42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.31.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HmqzMp_AoM0d"
      },
      "outputs": [],
      "source": [
        "# %load /content/internShalaProject/Yardstick_Assignment/getFunctionCallling.py\n",
        "import json\n",
        "\n",
        "def getJsonArg(\n",
        "        name : str,\n",
        "        email : str,\n",
        "        phone : str,\n",
        "        location : str,\n",
        "        age : int\n",
        "    ):\n",
        "\n",
        "    return_json = json.dumps({\n",
        "        \"name\": name,\n",
        "        \"email\": email,\n",
        "        \"phone\": phone,\n",
        "        \"location\": location,\n",
        "        \"age\": age\n",
        "    })\n",
        "\n",
        "    return return_json\n",
        "\n",
        "tools  = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"getJsonArg\",\n",
        "            \"description\": (\n",
        "                \"Captures and structures a user's complete contact and demographic information. \"\n",
        "                \"Use this function when a user needs to be registered, created in a system, or \"\n",
        "                \"have their full profile saved. It should only be called after all five details \"\n",
        "                \"(name, email, phone, location, age) have been provided.\"\n",
        "            ),\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"name\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The full name of the user mentioned in the chat.\"\n",
        "                    },\n",
        "                    \"email\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The email address provided by the user.\"\n",
        "                    },\n",
        "                    \"phone\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The contact or phone number of the user.\"\n",
        "                    },\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city or general location of the user.\"\n",
        "                    },\n",
        "                    \"age\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"The age of the user as an integer.\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"name\", \"email\", \"phone\", \"location\", \"age\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "available_function_tool_name = {'getJsonArg': getJsonArg }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VATA3Kshouox"
      },
      "outputs": [],
      "source": [
        "# from transformers import  AutoTokenizer\n",
        "import typing\n",
        "import json\n",
        "from groq import Groq\n",
        "from fastapi import FastAPI\n",
        "# from dotenv import load_dotenv\n",
        "from pydantic import BaseModel\n",
        "# from getFunctionCallling import available_function_tool_name, tools\n",
        "import os\n",
        "\n",
        "# app = FastAPI()\n",
        "class ChatCompletionMessageParam( BaseModel ):\n",
        "    role : str\n",
        "    content : typing.Union[dict[str, str], str ]\n",
        "    GroqApiKey: str\n",
        "\n",
        "\n",
        "\n",
        "class CreateSummary:\n",
        "    def __init__(self):\n",
        "        self.summary = []\n",
        "\n",
        "\n",
        "# tokenizer =  AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
        "maximum_tokens = 1000\n",
        "global_token_counter = 0\n",
        "keep_noOf_message = 20\n",
        "k_messages_feed_into_llm = 3\n",
        "max_token_posible_in_output = 1000\n",
        "\n",
        "if keep_noOf_message < k_messages_feed_into_llm:\n",
        "    raise RuntimeError(f'keep_noOf_message must be lower than k_messages_feed_into_llm  ( {keep_noOf_message} < {k_messages_feed_into_llm  }) ')\n",
        "\n",
        "def summarization_logic( summarizationClass : CreateSummary , client ):\n",
        "\n",
        "    for index, actual_content in enumerate(reversed(summarizationClass.summary)):\n",
        "        if index >= keep_noOf_message:\n",
        "            content = summarizationClass.summary[ index: ]\n",
        "        else:\n",
        "            content = actual_content\n",
        "\n",
        "        llm_response = client.chat.completions.create(\n",
        "            messages =[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Summarize the following text into the shortest possible version while preserving all essential \\\n",
        "                        information and eliminating redundancy. Ensure no important detail is lost. The result must be concise,\\\n",
        "                             precise, and information-dense. Text: {content}\"\n",
        "                }\n",
        "            ],\n",
        "            model = 'gemma2-9b-it'\n",
        "        )\n",
        "        response = llm_response.choices[0].message.content\n",
        "        summarizationClass.summary.append(response)\n",
        "\n",
        "_summary_class = CreateSummary()\n",
        "\n",
        "# @app.post('/v2/getResponse')\n",
        "def get_response(userquery : ChatCompletionMessageParam ):\n",
        "    global global_token_counter\n",
        "    global _summary_class\n",
        "    try:\n",
        "\n",
        "        client = Groq( api_key = userquery.GroqApiKey )\n",
        "        # input_tokens = tokenizer(userquery.content).input_ids\n",
        "        input_tokens = userquery.content\n",
        "\n",
        "        if (len(input_tokens) >= maximum_tokens):\n",
        "            return {\"error\": f\"input tokens >= {maximum_tokens}\"}\n",
        "        else:\n",
        "            global_token_counter += len(input_tokens)\n",
        "\n",
        "        if  ((global_token_counter + len(input_tokens)) >= maximum_tokens):\n",
        "            summarization_logic( summarizationClass= _summary_class , client = client)\n",
        "\n",
        "        if len(_summary_class.summary) <= k_messages_feed_into_llm:\n",
        "            content = f\"past summary of conversation : {_summary_class.summary} \\n\\n\" + f\"current user_qestion : {userquery.content}\"\n",
        "        else:\n",
        "            content = f\"past summary of conversation : {_summary_class.summary[:k_messages_feed_into_llm]} \\n\\n\" + f\"current user_qestion : {userquery.content}\"\n",
        "\n",
        "\n",
        "        llm_input_message = [\n",
        "                {\n",
        "                    \"role\": userquery.role,\n",
        "                    \"content\" : content\n",
        "                }\n",
        "            ]\n",
        "        response = client.chat.completions.create(\n",
        "            messages=llm_input_message,\n",
        "            model = 'gemma2-9b-it'\n",
        "        ).choices[0].message.content\n",
        "\n",
        "        # if len(tokenizer.tokenize(response)) >= max_token_posible_in_output:\n",
        "        if len(response) >= max_token_posible_in_output:\n",
        "\n",
        "            _o5th_path = int(len(response) / 5)\n",
        "            return_response = f'{response[:_o5th_path]} </---------/>  {response[-_o5th_path :]}'\n",
        "        else:\n",
        "            return_response = response\n",
        "\n",
        "        _summary_class.summary.append(f'userquery : {userquery.content}  llmRespose : {response}')\n",
        "        return {'response' : return_response }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"got error during getting response from llm due to : {e}\"}\n",
        "\n",
        "# @app.post('/v2/getJsonResponse')\n",
        "def get_json_response(userquery : ChatCompletionMessageParam ):\n",
        "    try:\n",
        "\n",
        "        client = Groq( api_key = userquery.GroqApiKey )\n",
        "\n",
        "        llm_input_message = [\n",
        "                {\n",
        "                    \"role\": userquery.role,\n",
        "                    \"content\" : userquery.content\n",
        "                }\n",
        "            ]\n",
        "        response = client.chat.completions.create(\n",
        "            messages=llm_input_message,\n",
        "            model = 'gemma2-9b-it',\n",
        "            tools= tools\n",
        "        )\n",
        "\n",
        "        # for now ,we are assume we have only one tool\n",
        "        for tool_metadata in response.choices[0].message.tool_calls:\n",
        "\n",
        "            tool_args_in_str  = tool_metadata.function.arguments\n",
        "            tool_args_in_json = json.loads(tool_args_in_str)\n",
        "\n",
        "            function_name = tool_metadata.function.name\n",
        "\n",
        "            function_object = available_function_tool_name[function_name]\n",
        "            function_response = function_object(**tool_args_in_json)\n",
        "\n",
        "            # if userquery.care_about_task2 is True:\n",
        "            return {\"args_for_next_function\": function_response }\n",
        "\n",
        "        #     json_format_function_response = json.dumps({\n",
        "        #         \"tool_call_id\": tool_metadata.id,\n",
        "        #         \"role\": \"tool\",\n",
        "        #         \"content\": function_response\n",
        "        #     })\n",
        "\n",
        "        #     llm_input_message.append( json.loads(json_format_function_response) )\n",
        "\n",
        "        # final_response = client.chat.completions.create(\n",
        "        #     messages= llm_input_message,\n",
        "        #     model= 'gemma2-9b-it'\n",
        "        # ).choices[0].message.content\n",
        "\n",
        "        # return {'json_response': final_response}\n",
        "    except Exception as e:\n",
        "        return {'error': f\"{e}\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = ChatCompletionMessageParam(\n",
        "    role = 'user',\n",
        "    content = 'my name is peter and my age is 19 and location is  abohar and phone number is 7717565570 email is gspeter67@gmail.com ',\n",
        "    GroqApiKey = os.getenv('GROQ_API_KEY')\n",
        "    )\n",
        "\n",
        "print(get_json_response(args))\n",
        "print(get_response(args))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXeFmU7XM_MX",
        "outputId": "e278b95c-a153-4a59-ef0c-33d80f4d9b71"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'args_for_next_function': '{\"name\": \"peter\", \"email\": \"gspeter67@gmail.com\", \"phone\": \"7717565570\", \"location\": \"abohar\", \"age\": 19}'}\n",
            "{'response': \"Please remember that sharing personal information like your phone number and email address online can be risky. It's important to protect your privacy. \\n\\nI understand you're telling me your name is Peter, you're 19 years old, you live in Abohar, and you gave me your email address and phone number.\\n\\nIs there anything else I can help you with? Perhaps you'd like to discuss something specific about Abohar, or maybe you have a question about something else entirely?  \\n\\n\"}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}